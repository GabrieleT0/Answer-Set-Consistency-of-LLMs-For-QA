{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89317569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def summarize_json_scores(folder):\n",
    "    \"\"\"\n",
    "    Reads all JSON files in the given folder and computes the average score for each file.\n",
    "    Assumes each JSON file is a dictionary of numeric values.\n",
    "    \n",
    "    Returns:\n",
    "        A pandas DataFrame with columns: Filename, Average Score, Count of 1s, Ratio of 1s\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if not filename.endswith(\".json\"):\n",
    "            continue\n",
    "\n",
    "        filepath = os.path.join(folder, filename)\n",
    "\n",
    "        with open(filepath, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        values = list(data.values())\n",
    "        if not values:\n",
    "            continue\n",
    "\n",
    "        avg_score = sum(values) / len(values)\n",
    "        count_ones = sum(1 for v in values if v == 1)\n",
    "        ratio_ones = count_ones / len(values)\n",
    "\n",
    "        rows.append({\n",
    "            \"Filename\": filename,\n",
    "            \"Average Score\": round(avg_score, 4),\n",
    "            \"Count of 1s\": count_ones,\n",
    "            \"Size\": len(values)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Example usage\n",
    "# df = summarize_json_scores(\"../data/evaluation_results/\")\n",
    "# print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "181ad737",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/answers/relation-classification/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/answers/relation-classification/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43msummarize_json_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df\n",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m, in \u001b[0;36msummarize_json_scores\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mReads all JSON files in the given folder and computes the average score for each file.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03mAssumes each JSON file is a dictionary of numeric values.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    A pandas DataFrame with columns: Filename, Average Score, Count of 1s, Ratio of 1s\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/answers/relation-classification/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "folder = '../data/answers/relation-classification/'\n",
    "df = summarize_json_scores(folder)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1864ee3d",
   "metadata": {},
   "source": [
    "### Evaluation Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13fd45b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Calculate the Jaccard similarity between two sets.\"\"\"\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "# For the subset-superset check\n",
    "def is_subset(set1, set2):\n",
    "    \"\"\"Check if set1 is a subset of set2.\"\"\"\n",
    "    return set1.issubset(set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c65423f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def compute_similarity(\n",
    "    model: str,\n",
    "    task: str,\n",
    "    action: str,\n",
    "    base_dir: str = '../data/answers',\n",
    "    output_dir: str = '.',\n",
    "    star = False\n",
    "):\n",
    "    if star:\n",
    "        prefix = '*'\n",
    "    else:\n",
    "        prefix = 'en_'\n",
    "    # Build file paths\n",
    "    action_word = \"\"\n",
    "    if action == \"wikidata\":\n",
    "        action_word = \"wikidata_\"\n",
    "    \n",
    "    ql1_path = os.path.join(base_dir, task, f'{prefix}ql1_{task}_answers_{action_word}{model}.json')\n",
    "    ql2_path = os.path.join(base_dir, task, f'{prefix}ql2_{task}_answers_{action_word}{model}.json')\n",
    "    \n",
    "    ql3_path = None\n",
    "    if task == \"minus\":\n",
    "        ql3_path = os.path.join(base_dir, task, f'{prefix}ql3_{task}_answers_{action_word}{model}.json')\n",
    "        ql1_path = os.path.join(base_dir, \"sup-sub\", f'{prefix}ql1_sup-sub_answers_{action_word}{model}.json')\n",
    "        ql2_path = os.path.join(base_dir, \"sup-sub\", f'{prefix}ql2_sup-sub_answers_{action_word}{model}.json')\n",
    "  \n",
    "    # Load data\n",
    "    try: \n",
    "        with open(ql1_path, 'r', encoding='utf-8') as f:\n",
    "            ql1_answers = json.load(f)\n",
    "        with open(ql2_path, 'r', encoding='utf-8') as f:\n",
    "            ql2_answers = json.load(f)\n",
    "        if ql3_path is not None:\n",
    "            with open(ql3_path, 'r', encoding='utf-8') as f:\n",
    "                ql3_answers = json.load(f)\n",
    "        else:\n",
    "            ql3_answers = None\n",
    "    except:\n",
    "        print(f\"Error loading files: {ql1_path}, {ql2_path}, {ql3_path}\")   \n",
    "        return None\n",
    "    # Compute metrics per question\n",
    "    similarity_scores = {}\n",
    "    if task == \"minus\":\n",
    "        # For minus task, we need to compare ql1 and ql2 with ql3\n",
    "        for qid, ans3 in ql3_answers.items():\n",
    "            set3 = set(ans3)\n",
    "            qid = str(int(qid) + 1)\n",
    "            set_a = set(ql1_answers.get(qid, []))\n",
    "            set_b = set(ql2_answers.get(qid, [])) \n",
    "            set_c =  set_b - set_a\n",
    "            sim = jaccard_similarity(set3, set_c)\n",
    "            is_empty = int(len(set3) == 0 and len(set_c) == 0)\n",
    "            binary_eq = int(set_c == set3)\n",
    "            similarity_scores[qid] = (sim, is_empty, binary_eq)\n",
    "    else:\n",
    "        for qid, ans1 in ql1_answers.items():\n",
    "            set1 = set(ans1)\n",
    "            set2 = set(ql2_answers.get(qid, []))\n",
    "            sim = jaccard_similarity(set1, set2)\n",
    "            is_empty = int(len(set1) == 0 and len(set2) == 0)\n",
    "            binary_eq = int(set1 == set2)\n",
    "            similarity_scores[qid] = (sim, is_empty, binary_eq)\n",
    "\n",
    "    # Create DataFrame\n",
    "    sim_df = pd.DataFrame.from_dict(\n",
    "        similarity_scores,\n",
    "        orient='index',\n",
    "        columns=['JaccardSimilarity', 'IsEmptySet', 'BinaryEqual']\n",
    "    )\n",
    "\n",
    "    # Ensure index is a column\n",
    "    sim_df = sim_df.reset_index().rename(columns={'index': 'QuestionID'})\n",
    "\n",
    "    # Save to TSV\n",
    "    output_filename = f'{prefix}{task}_{model}_{action}.tsv'\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    sim_df.to_csv(output_path, sep='\\t', index=False)\n",
    "    print(f\"Saved similarity results to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a280277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved similarity results to ../data/evaluation_results/en_equal_CustomServerLLM_original.tsv\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "model = ['CustomServerLLM']\n",
    "task = ['equal']\n",
    "action = [\"original\"]\n",
    "\n",
    "for m in model:\n",
    "    for t in task:\n",
    "        for a in action:\n",
    "            compute_similarity(\n",
    "                model=m,\n",
    "                task=t,\n",
    "                action=a,\n",
    "                base_dir='../data/answers/zero-shot',\n",
    "                output_dir='../data/evaluation_results',\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78492763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Model</th>\n",
       "      <th>Action</th>\n",
       "      <th>Average_All</th>\n",
       "      <th>Average_NoEmpty</th>\n",
       "      <th>Ratio_empty</th>\n",
       "      <th>Binary_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*equal</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>0.7593</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>0.4800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*equal</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>wikidata</td>\n",
       "      <td>0.5143</td>\n",
       "      <td>0.7564</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.4400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>*equal</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.5319</td>\n",
       "      <td>0.7822</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*equal</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>wikidata</td>\n",
       "      <td>0.3355</td>\n",
       "      <td>0.6989</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>0.4200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*minus</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.1652</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.0526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>*minus</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>wikidata</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>0.1316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>*minus</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.3067</td>\n",
       "      <td>0.8947</td>\n",
       "      <td>0.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>*minus</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>wikidata</td>\n",
       "      <td>0.0627</td>\n",
       "      <td>0.5958</td>\n",
       "      <td>0.8947</td>\n",
       "      <td>0.2632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>*sup-sub</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.3081</td>\n",
       "      <td>0.5090</td>\n",
       "      <td>0.3947</td>\n",
       "      <td>0.1842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>*sup-sub</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>wikidata</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>0.5526</td>\n",
       "      <td>0.1579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>*sup-sub</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.5248</td>\n",
       "      <td>0.5789</td>\n",
       "      <td>0.2105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>*sup-sub</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>wikidata</td>\n",
       "      <td>0.1879</td>\n",
       "      <td>0.6492</td>\n",
       "      <td>0.7105</td>\n",
       "      <td>0.2895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>equal</td>\n",
       "      <td>CustomServerLLM</td>\n",
       "      <td>original</td>\n",
       "      <td>0.3082</td>\n",
       "      <td>0.5581</td>\n",
       "      <td>0.4477</td>\n",
       "      <td>0.1727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>equal</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.5706</td>\n",
       "      <td>0.7022</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.3716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>equal</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>wikidata</td>\n",
       "      <td>0.4825</td>\n",
       "      <td>0.7624</td>\n",
       "      <td>0.3670</td>\n",
       "      <td>0.4830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>equal</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.4552</td>\n",
       "      <td>0.6503</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.3057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>equal</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>wikidata</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>0.7900</td>\n",
       "      <td>0.7534</td>\n",
       "      <td>0.5966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>minus</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.1917</td>\n",
       "      <td>0.4553</td>\n",
       "      <td>0.5789</td>\n",
       "      <td>0.1316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>minus</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>wikidata</td>\n",
       "      <td>0.1985</td>\n",
       "      <td>0.4438</td>\n",
       "      <td>0.5526</td>\n",
       "      <td>0.2632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>minus</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.1844</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>0.6579</td>\n",
       "      <td>0.1579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>minus</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>wikidata</td>\n",
       "      <td>0.1252</td>\n",
       "      <td>0.6798</td>\n",
       "      <td>0.8158</td>\n",
       "      <td>0.3947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sup-sub</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.3833</td>\n",
       "      <td>0.3077</td>\n",
       "      <td>0.0513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sup-sub</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>wikidata</td>\n",
       "      <td>0.2571</td>\n",
       "      <td>0.4558</td>\n",
       "      <td>0.4359</td>\n",
       "      <td>0.1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sup-sub</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.2119</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.0769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sup-sub</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>wikidata</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.9487</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Task                    Model    Action  Average_All  Average_NoEmpty  \\\n",
       "0     *equal  gpt-4.1-mini-2025-04-14  original       0.5619           0.7593   \n",
       "1     *equal  gpt-4.1-mini-2025-04-14  wikidata       0.5143           0.7564   \n",
       "2     *equal  gpt-4.1-nano-2025-04-14  original       0.5319           0.7822   \n",
       "3     *equal  gpt-4.1-nano-2025-04-14  wikidata       0.3355           0.6989   \n",
       "4     *minus  gpt-4.1-mini-2025-04-14  original       0.0087           0.1652   \n",
       "5     *minus  gpt-4.1-mini-2025-04-14  wikidata       0.0197           0.3750   \n",
       "6     *minus  gpt-4.1-nano-2025-04-14  original       0.0323           0.3067   \n",
       "7     *minus  gpt-4.1-nano-2025-04-14  wikidata       0.0627           0.5958   \n",
       "8   *sup-sub  gpt-4.1-mini-2025-04-14  original       0.3081           0.5090   \n",
       "9   *sup-sub  gpt-4.1-mini-2025-04-14  wikidata       0.2268           0.5070   \n",
       "10  *sup-sub  gpt-4.1-nano-2025-04-14  original       0.2210           0.5248   \n",
       "11  *sup-sub  gpt-4.1-nano-2025-04-14  wikidata       0.1879           0.6492   \n",
       "12     equal          CustomServerLLM  original       0.3082           0.5581   \n",
       "13     equal  gpt-4.1-mini-2025-04-14  original       0.5706           0.7022   \n",
       "14     equal  gpt-4.1-mini-2025-04-14  wikidata       0.4825           0.7624   \n",
       "15     equal  gpt-4.1-nano-2025-04-14  original       0.4552           0.6503   \n",
       "16     equal  gpt-4.1-nano-2025-04-14  wikidata       0.1948           0.7900   \n",
       "17     minus  gpt-4.1-mini-2025-04-14  original       0.1917           0.4553   \n",
       "18     minus  gpt-4.1-mini-2025-04-14  wikidata       0.1985           0.4438   \n",
       "19     minus  gpt-4.1-nano-2025-04-14  original       0.1844           0.5391   \n",
       "20     minus  gpt-4.1-nano-2025-04-14  wikidata       0.1252           0.6798   \n",
       "21   sup-sub  gpt-4.1-mini-2025-04-14  original       0.2654           0.3833   \n",
       "22   sup-sub  gpt-4.1-mini-2025-04-14  wikidata       0.2571           0.4558   \n",
       "23   sup-sub  gpt-4.1-nano-2025-04-14  original       0.2119           0.4590   \n",
       "24   sup-sub  gpt-4.1-nano-2025-04-14  wikidata       0.0427           0.8333   \n",
       "\n",
       "    Ratio_empty  Binary_count  \n",
       "0        0.2600        0.4800  \n",
       "1        0.3200        0.4400  \n",
       "2        0.3200        0.4600  \n",
       "3        0.5200        0.4200  \n",
       "4        0.9474        0.0526  \n",
       "5        0.9474        0.1316  \n",
       "6        0.8947        0.1053  \n",
       "7        0.8947        0.2632  \n",
       "8        0.3947        0.1842  \n",
       "9        0.5526        0.1579  \n",
       "10       0.5789        0.2105  \n",
       "11       0.7105        0.2895  \n",
       "12       0.4477        0.1727  \n",
       "13       0.1875        0.3716  \n",
       "14       0.3670        0.4830  \n",
       "15       0.3000        0.3057  \n",
       "16       0.7534        0.5966  \n",
       "17       0.5789        0.1316  \n",
       "18       0.5526        0.2632  \n",
       "19       0.6579        0.1579  \n",
       "20       0.8158        0.3947  \n",
       "21       0.3077        0.0513  \n",
       "22       0.4359        0.1282  \n",
       "23       0.5385        0.0769  \n",
       "24       0.9487        0.3333  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def summarize_evaluation_results(folder):\n",
    "    \"\"\"\n",
    "    Summarize per-question TSV evaluation files in a folder.\n",
    "    Expects filenames in the format: task-model-action.tsv\n",
    "    and each TSV contains columns: QuestionID, JaccardSimilarity, IsEmptySet, BinaryEqual\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if not filename.endswith(\".tsv\"):\n",
    "            continue\n",
    "\n",
    "        # Extract metadata from filename\n",
    "        base = filename[:-4]  # remove .tsv\n",
    "        parts = base.split('_')\n",
    "        task = parts[0]\n",
    "        action = parts[-1]\n",
    "        model = parts[1]\n",
    "\n",
    "        # Read TSV\n",
    "        df = pd.read_csv(os.path.join(folder, filename), sep='\\t')\n",
    "\n",
    "        # Ensure JaccardSimilarity column exists\n",
    "        if 'JaccardSimilarity' not in df.columns:\n",
    "            raise ValueError(f\"Missing 'JaccardSimilarity' in {filename}\")\n",
    "\n",
    "        # Compute overall average\n",
    "        avg_all = df['JaccardSimilarity'].mean()\n",
    "\n",
    "        # Identify empty (zero) Jaccard entries\n",
    "        is_empty = df['JaccardSimilarity'] == 0\n",
    "        ratio_empty = is_empty.mean()\n",
    "\n",
    "        # Average excluding empty entries\n",
    "        non_empty = df.loc[~is_empty, 'JaccardSimilarity']\n",
    "        avg_non_empty = non_empty.mean() if not non_empty.empty else float('nan')\n",
    "\n",
    "        # Binary count: proportion where BinaryEqual == 1\n",
    "        if 'BinaryEqual' in df.columns:\n",
    "            binary_count = (df['BinaryEqual'] == 1).mean()\n",
    "        else:\n",
    "            binary_count = float('nan')\n",
    "\n",
    "        rows.append({\n",
    "            \"Task\": task,\n",
    "            \"Model\": model,\n",
    "            \"Action\": action,\n",
    "            \"Average_All\": round(avg_all, 4),\n",
    "            \"Average_NoEmpty\": round(avg_non_empty, 4),\n",
    "            \"Ratio_empty\": round(ratio_empty, 4),\n",
    "            \"Binary_count\": round(binary_count, 4),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Example usage:\n",
    "df = summarize_evaluation_results(\"../data/evaluation_results/\")\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b1220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
