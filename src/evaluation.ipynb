{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1864ee3d",
   "metadata": {},
   "source": [
    "### Evaluation Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13fd45b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    \"\"\"Calculate the Jaccard similarity between two sets.\"\"\"\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "# For the subset-superset check\n",
    "def is_subset(set1, set2):\n",
    "    \"\"\"Check if set1 is a subset of set2.\"\"\"\n",
    "    return set1.issubset(set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c65423f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def compute_similarity(\n",
    "    model: str,\n",
    "    task: str,\n",
    "    action: str,\n",
    "    base_dir: str = '../data/answers',\n",
    "    output_dir: str = '.',\n",
    "):\n",
    "\n",
    "    # Build file paths\n",
    "    action_word = \"\"\n",
    "    if action == \"wikidata\":\n",
    "        action_word = \"wikidata_\"\n",
    "    \n",
    "    ql1_path = os.path.join(base_dir, task, f'ql1_{task}_answers_{action_word}{model}.json')\n",
    "    ql2_path = os.path.join(base_dir, task, f'ql2_{task}_answers_{action_word}{model}.json')\n",
    "    \n",
    "    ql3_path = None\n",
    "    if task == \"minus\":\n",
    "        ql3_path = os.path.join(base_dir, task, f'ql3_{task}_answers_{action_word}{model}.json')\n",
    "        ql1_path = os.path.join(base_dir, \"sup-sub\", f'ql1_sup-sub_answers_{action_word}{model}.json')\n",
    "        ql2_path = os.path.join(base_dir, \"sup-sub\", f'ql2_sup-sub_answers_{action_word}{model}.json')\n",
    "  \n",
    "    # Load data\n",
    "    try: \n",
    "        with open(ql1_path, 'r', encoding='utf-8') as f:\n",
    "            ql1_answers = json.load(f)\n",
    "        with open(ql2_path, 'r', encoding='utf-8') as f:\n",
    "            ql2_answers = json.load(f)\n",
    "        if ql3_path is not None:\n",
    "            with open(ql3_path, 'r', encoding='utf-8') as f:\n",
    "                ql3_answers = json.load(f)\n",
    "        else:\n",
    "            ql3_answers = None\n",
    "    except:\n",
    "        print(f\"Error loading files: {ql1_path}, {ql2_path}, {ql3_path}\")   \n",
    "        return None\n",
    "    # Compute metrics per question\n",
    "    similarity_scores = {}\n",
    "    if task == \"minus\":\n",
    "        # For minus task, we need to compare ql1 and ql2 with ql3\n",
    "        for qid, ans3 in ql3_answers.items():\n",
    "            set3 = set(ans3)\n",
    "            qid = str(int(qid) + 1)\n",
    "            set_a = set(ql1_answers[qid])\n",
    "            set_b = set(ql2_answers.get(qid, [])) \n",
    "            set_c = set_a - set_b\n",
    "            sim = jaccard_similarity(set3, set_c)\n",
    "            is_empty = int(len(set3) == 0 and len(set_c) == 0)\n",
    "            binary_eq = int(set_c == set3)\n",
    "            similarity_scores[qid] = (sim, is_empty, binary_eq)\n",
    "    else:\n",
    "        for qid, ans1 in ql1_answers.items():\n",
    "            set1 = set(ans1)\n",
    "            set2 = set(ql2_answers.get(qid, []))\n",
    "            sim = jaccard_similarity(set1, set2)\n",
    "            is_empty = int(len(set1) == 0 and len(set2) == 0)\n",
    "            binary_eq = int(set1 == set2)\n",
    "            similarity_scores[qid] = (sim, is_empty, binary_eq)\n",
    "\n",
    "    # Create DataFrame\n",
    "    sim_df = pd.DataFrame.from_dict(\n",
    "        similarity_scores,\n",
    "        orient='index',\n",
    "        columns=['JaccardSimilarity', 'IsEmptySet', 'BinaryEqual']\n",
    "    )\n",
    "\n",
    "    # Ensure index is a column\n",
    "    sim_df = sim_df.reset_index().rename(columns={'index': 'QuestionID'})\n",
    "\n",
    "    # Save to TSV\n",
    "    output_filename = f'{task}_{model}_{action}.tsv'\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    sim_df.to_csv(output_path, sep='\\t', index=False)\n",
    "    print(f\"Saved similarity results to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a280277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved similarity results to ../data/evaluation_results/equal_gpt-4.1-nano-2025-04-14_original.tsv\n",
      "Error loading files: ../data/answers/equal/ql1_equal_answers_wikidata_gpt-4.1-nano-2025-04-14.json, ../data/answers/equal/ql2_equal_answers_wikidata_gpt-4.1-nano-2025-04-14.json, None\n",
      "Saved similarity results to ../data/evaluation_results/sup-sub_gpt-4.1-nano-2025-04-14_original.tsv\n",
      "Saved similarity results to ../data/evaluation_results/sup-sub_gpt-4.1-nano-2025-04-14_wikidata.tsv\n",
      "Saved similarity results to ../data/evaluation_results/minus_gpt-4.1-nano-2025-04-14_original.tsv\n",
      "Saved similarity results to ../data/evaluation_results/minus_gpt-4.1-nano-2025-04-14_wikidata.tsv\n",
      "Saved similarity results to ../data/evaluation_results/equal_gpt-4.1-mini-2025-04-14_original.tsv\n",
      "Error loading files: ../data/answers/equal/ql1_equal_answers_wikidata_gpt-4.1-mini-2025-04-14.json, ../data/answers/equal/ql2_equal_answers_wikidata_gpt-4.1-mini-2025-04-14.json, None\n",
      "Saved similarity results to ../data/evaluation_results/sup-sub_gpt-4.1-mini-2025-04-14_original.tsv\n",
      "Error loading files: ../data/answers/sup-sub/ql1_sup-sub_answers_wikidata_gpt-4.1-mini-2025-04-14.json, ../data/answers/sup-sub/ql2_sup-sub_answers_wikidata_gpt-4.1-mini-2025-04-14.json, None\n",
      "Saved similarity results to ../data/evaluation_results/minus_gpt-4.1-mini-2025-04-14_original.tsv\n",
      "Error loading files: ../data/answers/sup-sub/ql1_sup-sub_answers_wikidata_gpt-4.1-mini-2025-04-14.json, ../data/answers/sup-sub/ql2_sup-sub_answers_wikidata_gpt-4.1-mini-2025-04-14.json, ../data/answers/minus/ql3_minus_answers_wikidata_gpt-4.1-mini-2025-04-14.json\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "model = ['gpt-4.1-nano-2025-04-14',\"gpt-4.1-mini-2025-04-14\"]\n",
    "task = ['equal', 'sup-sub', \"minus\"]\n",
    "action = [\"original\",\"wikidata\"]\n",
    "\n",
    "for m in model:\n",
    "    for t in task:\n",
    "        for a in action:\n",
    "            compute_similarity(\n",
    "                model=m,\n",
    "                task=t,\n",
    "                action=a,\n",
    "                base_dir='../data/answers',\n",
    "                output_dir='../data/evaluation_results',\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78492763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Model</th>\n",
       "      <th>Action</th>\n",
       "      <th>Average_All</th>\n",
       "      <th>Average_NoEmpty</th>\n",
       "      <th>Ratio_empty</th>\n",
       "      <th>Binary_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>equal</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.5706</td>\n",
       "      <td>0.7022</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.3716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>equal</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.4552</td>\n",
       "      <td>0.6503</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.3057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minus</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.1324</td>\n",
       "      <td>0.9211</td>\n",
       "      <td>0.0789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>minus</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.9737</td>\n",
       "      <td>0.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>minus</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>wikidata</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.5789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sup-sub</td>\n",
       "      <td>gpt-4.1-mini-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.3833</td>\n",
       "      <td>0.3077</td>\n",
       "      <td>0.0513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sup-sub</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>original</td>\n",
       "      <td>0.2119</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.0769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sup-sub</td>\n",
       "      <td>gpt-4.1-nano-2025-04-14</td>\n",
       "      <td>wikidata</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.9487</td>\n",
       "      <td>0.3333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Task                    Model    Action  Average_All  Average_NoEmpty  \\\n",
       "0    equal  gpt-4.1-mini-2025-04-14  original       0.5706           0.7022   \n",
       "1    equal  gpt-4.1-nano-2025-04-14  original       0.4552           0.6503   \n",
       "2    minus  gpt-4.1-mini-2025-04-14  original       0.0105           0.1324   \n",
       "3    minus  gpt-4.1-nano-2025-04-14  original       0.0033           0.1250   \n",
       "4    minus  gpt-4.1-nano-2025-04-14  wikidata       0.0000              NaN   \n",
       "5  sup-sub  gpt-4.1-mini-2025-04-14  original       0.2654           0.3833   \n",
       "6  sup-sub  gpt-4.1-nano-2025-04-14  original       0.2119           0.4590   \n",
       "7  sup-sub  gpt-4.1-nano-2025-04-14  wikidata       0.0427           0.8333   \n",
       "\n",
       "   Ratio_empty  Binary_count  \n",
       "0       0.1875        0.3716  \n",
       "1       0.3000        0.3057  \n",
       "2       0.9211        0.0789  \n",
       "3       0.9737        0.1053  \n",
       "4       1.0000        0.5789  \n",
       "5       0.3077        0.0513  \n",
       "6       0.5385        0.0769  \n",
       "7       0.9487        0.3333  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def summarize_evaluation_results(folder):\n",
    "    \"\"\"\n",
    "    Summarize per-question TSV evaluation files in a folder.\n",
    "    Expects filenames in the format: task-model-action.tsv\n",
    "    and each TSV contains columns: QuestionID, JaccardSimilarity, IsEmptySet, BinaryEqual\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if not filename.endswith(\".tsv\"):\n",
    "            continue\n",
    "\n",
    "        # Extract metadata from filename\n",
    "        base = filename[:-4]  # remove .tsv\n",
    "        parts = base.split('_')\n",
    "        task = parts[0]\n",
    "        action = parts[-1]\n",
    "        model = parts[1]\n",
    "\n",
    "        # Read TSV\n",
    "        df = pd.read_csv(os.path.join(folder, filename), sep='\\t')\n",
    "\n",
    "        # Ensure JaccardSimilarity column exists\n",
    "        if 'JaccardSimilarity' not in df.columns:\n",
    "            raise ValueError(f\"Missing 'JaccardSimilarity' in {filename}\")\n",
    "\n",
    "        # Compute overall average\n",
    "        avg_all = df['JaccardSimilarity'].mean()\n",
    "\n",
    "        # Identify empty (zero) Jaccard entries\n",
    "        is_empty = df['JaccardSimilarity'] == 0\n",
    "        ratio_empty = is_empty.mean()\n",
    "\n",
    "        # Average excluding empty entries\n",
    "        non_empty = df.loc[~is_empty, 'JaccardSimilarity']\n",
    "        avg_non_empty = non_empty.mean() if not non_empty.empty else float('nan')\n",
    "\n",
    "        # Binary count: proportion where BinaryEqual == 1\n",
    "        if 'BinaryEqual' in df.columns:\n",
    "            binary_count = (df['BinaryEqual'] == 1).mean()\n",
    "        else:\n",
    "            binary_count = float('nan')\n",
    "\n",
    "        rows.append({\n",
    "            \"Task\": task,\n",
    "            \"Model\": model,\n",
    "            \"Action\": action,\n",
    "            \"Average_All\": round(avg_all, 4),\n",
    "            \"Average_NoEmpty\": round(avg_non_empty, 4),\n",
    "            \"Ratio_empty\": round(ratio_empty, 4),\n",
    "            \"Binary_count\": round(binary_count, 4),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Example usage:\n",
    "df = summarize_evaluation_results(\"../data/evaluation_results/\")\n",
    "df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
